{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the nitrate database compilation. It is centered around Codispoti et al.'s 2013 compilation of historical nitrate measurements in the Arctic Ocean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All figures exported from this notebook are prefixed with `FIGURE_NO3-COMP_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run imports.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Define dimensions\n",
    "dims = dict(\n",
    "    reg_name=hv.Dimension('reg_name', label='Region'),\n",
    "    ntr0=hv.Dimension('ntr0', label='Surface NOâ‚ƒ conc.', range=(0,13)),\n",
    "    doy=hv.Dimension('doy', label='Day of the year')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define individual datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Codispoti et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the biggest database of vertical profiles nitrate of nitrate concentrations in the Arctic known to us: The one compiled by Codispoti and collaborators and published in 2013: http://dx.doi.org/10.1016/j.pocean.2012.11.006 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -c --read-timeout=5 -P ../data/no3-compilation/ \"https://www.nodc.noaa.gov/archive/arc0034/0072133/1.1/data/0-data/Codispoti_Arctic_Nutrients_Submission_11-11-2010.csv\"\n",
    "\n",
    "def load_codispotietal():\n",
    "    \"\"\"\n",
    "    Codispoti et al., 2013\n",
    "    \"\"\"\n",
    "    renamedict = dict(NO3='nitrate', Sal='sal', \n",
    "                  T='temp', z='depth', Longitude='lon', \n",
    "                  Latitude='lat', Date='date', Station='station', Cruise='cruise'\n",
    "                 )\n",
    "\n",
    "    df = (pd.read_csv('../data/no3-compilation/Codispoti_Arctic_Nutrients_Submission_11-11-2010.csv',\n",
    "                      na_values=-999,\n",
    "                      parse_dates=['Date'], dtype={'Station': str},\n",
    "                     )[['Date','Latitude','Longitude','NO3','z','T','Sal','Station','Cruise']]\n",
    "           .rename(columns=renamedict)\n",
    "           .dropna(subset=['nitrate'])\n",
    "          )\n",
    "    df = df.assign(station=lambda row: row.cruise+'+'+row.station.astype(str))\n",
    "    df = df.drop(columns=['cruise'])\n",
    "    df = df.groupby(['station', 'date', 'depth']).mean().reset_index()\n",
    "\n",
    "    df['p'] = gsw.p_from_z(-df.depth, df.lat)\n",
    "    df['SA'] = gsw.SA_from_SP(df.sal, df.p, df.lon, df.lat)\n",
    "    df['CT'] = gsw.CT_from_pt(df.SA, df.temp)\n",
    "    df['sigth'] = gsw.sigma0(df.SA, df.CT)\n",
    "\n",
    "    df = df.assign(database='codispoti')\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canadian Arctic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data from the Canadian Arctic consist mostly of data collected during cruises of ArcticNet, DFO, and some others, and are treated in more detail by Pierre Coupel et al., 2019 (in prep.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_canadian_arctic():\n",
    "    fname = '../data/no3-compilation/NUT-ArcticNet_NOW_CATS_Coupel_reduced.csv'\n",
    "    if os.path.exists(fname):\n",
    "        df = (\n",
    "            pd.read_csv(fname, dtype={'Station': str})\n",
    "            .rename(\n",
    "                columns=dict(\n",
    "                    Station='station', Depth='depth', Longitude='lon', Latitude='lat', \n",
    "                    Nitrate='nitrate', Temperature='temp', Salinity='sal'\n",
    "                )\n",
    "            )\n",
    "            .dropna(subset=['nitrate'])\n",
    "        )\n",
    "\n",
    "        df.Day = df.Day.fillna(15)\n",
    "        df['date'] = pd.to_datetime(df.Year*10000+df.Month*100+df.Day, format='%Y%m%d')\n",
    "\n",
    "        df = df.assign(station=lambda row: row.station.astype(str))\n",
    "\n",
    "        df = df.groupby(['station', 'date', 'depth']).mean().reset_index()\n",
    "\n",
    "        df['p'] = gsw.p_from_z(-df.depth, df.lat)\n",
    "        df['SA'] = gsw.SA_from_SP(df.sal, df.p, df.lon, df.lat)\n",
    "        df['CT'] = gsw.CT_from_pt(df.SA, df.temp)\n",
    "        df['sigth'] = gsw.sigma0(df.SA, df.CT)\n",
    "\n",
    "        df = df.drop(columns=['Year', 'Month', 'Day'])\n",
    "        df = df.assign(database='arcticnet')\n",
    "        # conform longitudes to interval -180, 180\n",
    "        df['lon'] = df.lon.where(df.lon<=180, df.lon-360)\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        print('Data not available locally')\n",
    "        return pd.DataFrame(columns=[\n",
    "            'station', 'date', 'depth', 'lon', 'lat', \n",
    "            'temp', 'sal', 'nitrate', 'p',\n",
    "            'SA', 'CT', 'sigth', 'database'\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrigo et al., 2017, SUBICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winter data are always hard to come by, so let's get some more pre-bloom nitrate concentrations from the Chukchi Sea measured by Arrigo and collaborators and published under https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2017JG003881. These are downloaded from https://datadryad.org/stash/dataset/doi:10.5061/dryad.fm7b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -c --read-timeout=5 -O ../data/no3-compilation/SUBICE_high_NO3_hy1.csv \"https://datadryad.org/stash/downloads/file_stream/29860\"\n",
    "\n",
    "def load_arrigoetal():\n",
    "    df = pd.read_csv('../data/no3-compilation/SUBICE_high_NO3_hy1.csv', \n",
    "                     na_values=-999, skiprows=9, header=[0,1],\n",
    "                     dtype={'STNNBR': str},\n",
    "                    ).replace(to_replace=-999, value=np.nan)\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "\n",
    "    df.DATE = df.DATE.apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "\n",
    "    renamedict = dict(\n",
    "        DATE='date',LATITUDE='lat', LONGITUDE='lon', NITRAT='nitrate', BTLDPTH='depth', \n",
    "        STNNBR='station', CTDTMP='temp', CTDSAL='sal',\n",
    "    )\n",
    "\n",
    "    df = df.rename(columns=renamedict)\n",
    "    \n",
    "    df['p'] = gsw.p_from_z(-df.depth, df.lat)\n",
    "    df['SA'] = gsw.SA_from_SP(df.sal, df.p, df.lon, df.lat)\n",
    "    df['CT'] = gsw.CT_from_pt(df.SA, df.temp)\n",
    "    df['sigth'] = gsw.sigma0(df.SA, df.CT)\n",
    "    \n",
    "    return df[list(renamedict.values())+['p', 'SA', 'CT', 'sigth']].assign(database='arrigoetal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and postprocess data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all nutrient databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this for duplicates. First, granularize the longitudes and latitudes for comparison, then check that no profiles overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timelatlon_bin(df):\n",
    "    df['lonbin'] = pd.to_numeric(pd.cut(df.lon, np.arange(-180, 180, 0.1), labels=0.05+np.arange(-180, 180, 0.1)[:-1]))\n",
    "    df['latbin'] = pd.to_numeric(pd.cut(df.lat, np.arange(60, 90, 0.05), labels=0.025+np.arange(60, 90, 0.05)[:-1]))\n",
    "\n",
    "    # one representative entry for each profile\n",
    "    return df.groupby(['date', 'lonbin', 'latbin']).first()\n",
    "\n",
    "stns_canadian_arctic = get_timelatlon_bin(load_canadian_arctic())\n",
    "stns_codispotietal = get_timelatlon_bin(load_codispotietal())\n",
    "\n",
    "canadian_arctic_stations_to_merge = stns_canadian_arctic.station.loc[\n",
    "    stns_canadian_arctic.index\n",
    "    .drop(\n",
    "        stns_codispotietal.index,\n",
    "        errors='ignore',\n",
    "    )\n",
    "].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can select those data from the Canadian Arctic that will not duplicate any data that's already in the Codispoti et al. database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canadian_arctic = load_canadian_arctic()\n",
    "canadian_arctic_to_merge = canadian_arctic.loc[canadian_arctic.station.isin(canadian_arctic_stations_to_merge)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes us drop this many records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(canadian_arctic) - len(canadian_arctic_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = [\n",
    "    load_codispotietal(),\n",
    "    load_arrigoetal(),\n",
    "    canadian_arctic_to_merge\n",
    "]\n",
    "\n",
    "df = pd.concat(dfl, sort=False).reset_index()\n",
    "df.to_pickle('../data/no3-compilation/tmp_compiled.pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station-wise depth interpolation [This step takes some time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/no3-compilation/tmp_compiled.pandas')\n",
    "\n",
    "def groupwise_interp(df):\n",
    "    if df.depth.min()>15:\n",
    "        return None\n",
    "    else:\n",
    "        bins = np.arange(-1,301.1,2)\n",
    "        labels = bins[:-1]+np.diff(bins)/2\n",
    "        df.depth = pd.to_numeric(pd.cut(df.depth, bins=bins, labels=labels))\n",
    "        # remove nans, average profile, and re-index on the previous labels.\n",
    "        # this takes care of duplicate and missing depths\n",
    "        df = (df\n",
    "              .dropna(subset=['depth'])\n",
    "              .groupby('depth').mean()\n",
    "              .reindex(index=labels)\n",
    "             )\n",
    "        # first, interpolate values between depths\n",
    "        # then fill backwards ('bfill') to extrapolate to surface\n",
    "        return df.interpolate(method='linear', limit_area='inside').fillna(method='bfill').drop(\n",
    "            columns=['station', 'date', 'depth'], errors='ignore')\n",
    "    \n",
    "df = (df.groupby(['database', 'station', 'date'])\n",
    "      .apply(groupwise_interp)\n",
    "      .reset_index()\n",
    "     )\n",
    "\n",
    "df.to_pickle('../data/no3-compilation/tmp_compiled-interpolated.pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive per-profile quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we derive a number of quantities for each profile. Not all of them made it into the final article, but we left them in here for further analyses that might pop up in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/no3-compilation/tmp_compiled-interpolated.pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surface nitrate concentration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge 0-15 m average nitrate conc. (i.e., surface ntr) into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    df.loc[df.depth<=15]\n",
    "    .groupby(['database', 'station'], as_index=False)\n",
    "    .nitrate.mean()\n",
    "    .rename(columns={'nitrate': 'ntr0'}),\n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ml (sigth, depth, delta_sigth_crit=0.1):\n",
    "    \"\"\"\n",
    "    Find mixed layer with sfc. density+0.1 kg/m3 criterion.\n",
    "    \"\"\"\n",
    "    index = np.where(sigth>np.nanmean(sigth.iloc[:5]) + delta_sigth_crit)[0]\n",
    "    if len(index)>0:\n",
    "        return depth.iloc[index[0]].astype(float)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df = df.merge(pd.DataFrame(\n",
    "              df.groupby(df.station).apply(lambda g: find_ml(g.sigth, g.depth)),\n",
    "              columns=['mld']),on='station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nitracline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two relevant nitraclines:\n",
    "1. `nc0`: Biologically meaningful, when NO3 jumps over say 1uM, as it indicates the zone of nitrate depletion (if there is one)\n",
    "1. `nc`: Physically meaningful, when NO3 jumps over say sfc.NO3+1uM, as it indicates water mass transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nitracline0(no3, depth, no3crit=1.0):\n",
    "    index = np.where(no3>=no3crit)[0]\n",
    "    if len(index)>0:\n",
    "        return depth.iloc[index[0]].astype(float)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def find_nitracline(no3, depth):\n",
    "    no3sfc = no3.loc[depth<=10].mean()\n",
    "    index = np.where(no3>no3sfc+1.)[0]\n",
    "    if len(index)>0:\n",
    "        return depth.iloc[index[0]].astype(float)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "gb = df.groupby(df.station)\n",
    "df = df.merge(pd.DataFrame(\n",
    "    dict(nc0=gb.apply(lambda g: find_nitracline0(g.nitrate, g.depth, 1)),\n",
    "         nc =gb.apply(lambda g: find_nitracline(g.nitrate, g.depth))\n",
    "        )),on='station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40-100 m density difference\n",
    "\n",
    "sigth0 = df.loc[df.depth.between(35, 45)].groupby('station').mean().sigth\n",
    "sigth_deep = df.loc[df.depth.between(95, 105)].groupby('station').mean().sigth\n",
    "\n",
    "df = df.merge(pd.DataFrame(\n",
    "    dict(delta_sigth=(sigth_deep-sigth0).values, station=sigth0.index.values)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buoyancy_freq(sigma, depth, from_depth, layer_thickness=30):\n",
    "    \"\"\"\n",
    "    Calculate buoyancy frequency\n",
    "    over depth range [from_depth, from_depth+layer_thickness].\n",
    "    \"\"\"\n",
    "    nc_depth_range = (from_depth<=depth) & (depth<=from_depth+layer_thickness)\n",
    "    if not np.any(nc_depth_range):\n",
    "        return np.nan\n",
    "    else:\n",
    "        def slope(x, y, x_range):\n",
    "            return np.polyfit(x.loc[x_range], y.loc[x_range].sort_values(), 1)[0]\n",
    "\n",
    "        return 9.81/(1e3+np.mean(sigma)) * slope(depth, sigma, nc_depth_range)\n",
    "\n",
    "gb = df.groupby(df.station)\n",
    "df = df.merge(pd.DataFrame(\n",
    "    dict(\n",
    "        N2_30_60=gb.apply(lambda g: buoyancy_freq(g.sigth, g.depth, g.mld, 30))\n",
    "    )),\n",
    "    on='station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a smaller dataframe that only contains quantities that are independent of depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_station = df.groupby(['database', 'station', 'date'], as_index=False)[['lat', 'lon', 'ntr0', 'mld', 'nc0', 'nc', 'delta_sigth', 'N2_30_60']].first()\n",
    "\n",
    "df_per_station.to_pickle('../data/no3-compilation/tmp_compiled-interpolated-derived-per-station.pandas')\n",
    "df.to_pickle('../data/no3-compilation/tmp_compiled-interpolated-derived.pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off Peralta-Ferriz & Woodgate and Codispoti et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [\n",
    "    ('Chukchi Sea', smoothen(box(-180, 68, -155, 76))), \n",
    "    ('Southern Beaufort', smoothen(box(-155, 68, -115, 72))), \n",
    "    ('Canada Basin', smoothen(box(-155, 72, -130, 84))), \n",
    "    ('Makarov Basin', smoothen(box(-180, 83.5, -50, 90)).union(smoothen(box(140, 78, 180, 90)))), \n",
    "    ('Eurasian Basin', smoothen(box(-30, 82, 140, 90).union(box(110, 78, 140, 82)))), \n",
    "    ('Barents Sea', smoothen(box(15, 75, 60, 80).union(box(15, 70.5, 55, 75)))),\n",
    "    ('Baffin Bay', smoothen(box(-65, 66, -45, 78))),\n",
    "    ('Canadian Archipelago', smoothen(box(-110, 66, -65, 80))), #.union(box(-100, 78, -50, 82))))\n",
    "    ('Fram Strait (East)', smoothen(box(-5, 75, 15, 81)))\n",
    "]\n",
    "\n",
    "names, geo = zip(*d)\n",
    "\n",
    "regions = gpd.GeoDataFrame(dict(reg_name=list(names)), geometry=list(geo))\n",
    "regions.crs = from_epsg(4326)\n",
    "\n",
    "# regions = regions.to_crs(from_epsg(3413))\n",
    "\n",
    "regions['reg_idx'] = range(len(d))\n",
    "\n",
    "regions.to_file('../data/regions/arctic-regions.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '../nb_fig/FIGURE_NO3-COMP_regions'\n",
    "\n",
    "hv.extension('bokeh')\n",
    "\n",
    "regions = gpd.read_file('../data/regions/arctic-regions.shp')\n",
    "options_bk = [\n",
    "    opts.Overlay(legend_position='left'),\n",
    "    opts.Polygons(projection=ccrs.NorthPolarStereo(), cmap='Category10', tools=['hover'], \n",
    "                  show_legend=True, \n",
    "                  frame_width=500, aspect='equal',\n",
    "                  line_color=None, alpha=.7,\n",
    "                 ),\n",
    "]\n",
    "add_backend_to_opts(options_bk, 'bokeh')\n",
    "\n",
    "poly = gv.Polygons(regions, kdims=['Longitude', 'Latitude'], vdims=['reg_name', 'reg_idx'])\n",
    "l = poly *gf.land * gf.coastline * isobath2000.relabel('2000 m isobath')\n",
    "l = l.opts(*options_bk)\n",
    "\n",
    "hv.save(l.opts(toolbar=None), fname, fmt='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually fix matplotlib legend handler so we can have a holoviews Polygons legend\n",
    "\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.legend_handler import HandlerPolyCollection\n",
    "from matplotlib.legend import Legend\n",
    "\n",
    "Legend.update_default_handler_map({PatchCollection: HandlerPolyCollection()})\n",
    "\n",
    "hv.extension('matplotlib')\n",
    "regions = gpd.read_file('../data/regions/arctic-regions.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_mpl = [\n",
    "    opts.Polygons(\n",
    "        projection=ccrs.NorthPolarStereo(), \n",
    "        show_legend=True, color=None,\n",
    "    ),\n",
    "    opts.NdOverlay(show_legend=True),\n",
    "]\n",
    "add_backend_to_opts(options_mpl, 'matplotlib')\n",
    "\n",
    "poly = hv.NdOverlay({\n",
    "    d.reg_name: gv.Polygons(gpd.GeoDataFrame(d.to_frame().transpose()))\n",
    "    for k, d in regions.iterrows()\n",
    "})\n",
    "\n",
    "l = poly * gf.land * gf.coastline\n",
    "l = l.opts(*options_mpl).redim.range(Latitude=(60,90), Longitude=(-180, 180))\n",
    "\n",
    "# render to matplotlib figure\n",
    "fig = hv.render(l)\n",
    "\n",
    "# do final adjustments directly in matplotlib\n",
    "fig.set_size_inches(9, 6)\n",
    "\n",
    "ax = fig.axes[0]\n",
    "circumpolar_axis(ax)\n",
    "ax.legend_.set_bbox_to_anchor((1, 1))\n",
    "ax.text(0, 0.99, 'B', transform=ax.transAxes, fontdict={'size':20, 'weight': 'bold'})\n",
    "\n",
    "for fmt in ['.png', '.pdf']:\n",
    "    fig.savefig(fname+fmt)\n",
    "    \n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add ancillary vars and regionalize dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/no3-compilation/tmp_compiled-interpolated-derived-per-station.pandas')\n",
    "dfp = pd.read_pickle('../data/no3-compilation/tmp_compiled-interpolated-derived.pandas')\n",
    "\n",
    "def season(timestamp):\n",
    "    mm = timestamp.month\n",
    "    if mm<=4:\n",
    "        return 'winter'\n",
    "    elif mm>=7 and mm<=9:\n",
    "        return 'summer'\n",
    "    \n",
    "def add_info(df):\n",
    "    df = (df\n",
    "          .assign(doy=df.date.dt.dayofyear)\n",
    "          .assign(month=df.date.dt.month)\n",
    "          .assign(year=df.date.dt.year)\n",
    "     )\n",
    "    df['season'] = df.date.apply(season)\n",
    "    return df\n",
    "\n",
    "df = add_info(df)\n",
    "\n",
    "gdf = df_to_gdf(df)\n",
    "gdf = (gpd.sjoin(gdf, regions, op='within', how='left')\n",
    "        .reset_index()\n",
    "        .drop(columns=['index_right', 'index_left', 'index'], errors='ignore')\n",
    "       )\n",
    "df = pd.DataFrame(gdf).drop(columns=['geometry'])\n",
    "\n",
    "dfp = dfp.merge(df, how='outer')\n",
    "dfp = add_info(dfp)\n",
    "\n",
    "df.to_csv('../data/no3-compilation/database-per-stn.csv', index=False)\n",
    "dfp.to_csv('../data/no3-compilation/database.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
