{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the nitrate database compilation. It is centered around Codispoti et al.'s 2013 compilation of historical nitrate measurements in the Arctic Ocean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All figures exported from this notebook are prefixed with `FIGURE_FN-COMP_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run imports.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions\n",
    "dims = dict(\n",
    "    reg_name=hv.Dimension('reg_name', label='Region'),\n",
    "    ntr0=hv.Dimension('ntr0', label='Surface NOâ‚ƒ conc.', range=(0,13)),\n",
    "    doy=hv.Dimension('doy', label='Day of the year')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define individual datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Arrigo et al 2017: CTD+nut+chla:  `databases/arrigo2017`\n",
    "    - https://datadryad.org/resource/doi:10.5061/dryad.fm7b5\n",
    "    - https://doi.org/10.5061/dryad.fm7b5\n",
    "    \n",
    "- Pierre Coupel's compilation (ArcticNet+others): `databases/arcticnet-nutrients-pierre/`\n",
    "- Codispoti et al.'s compilation:  `databases/codispoti2013synthesis-1.1/` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common dataframe format:\n",
    "- station < cast no, station,...\n",
    "- date\n",
    "    - year\n",
    "    - month\n",
    "    - day\n",
    "- depth\n",
    "    - p\n",
    "- lon\n",
    "- lat\n",
    "- temperature\n",
    "    - CT\n",
    "- salinity\n",
    "    - SA\n",
    "- sigth\n",
    "- nitrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Codispoti et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_codispotietal():\n",
    "    \"\"\"\n",
    "    Codispoti et al., 2013\n",
    "    \"\"\"\n",
    "    renamedict = dict(NO3='nitrate', Sal='sal', \n",
    "                  T='temp', z='depth', Longitude='lon', \n",
    "                  Latitude='lat', Date='date', Station='station', Cruise='cruise'\n",
    "                 )\n",
    "\n",
    "    df = (pd.read_csv('/Users/doppler/database/codispoti2013synthesis-1.1/data/0-data/Codispoti_Arctic_Nutrients_Submission_11-11-2010.csv',\n",
    "                      na_values=-999,\n",
    "                      parse_dates=['Date']\n",
    "                     )[['Date','Latitude','Longitude','NO3','z','T','Sal','Station','Cruise']]\n",
    "           .rename(columns=renamedict)\n",
    "           .dropna(subset=['nitrate'])\n",
    "          )\n",
    "    df = df.assign(station=lambda row: row.cruise+'+'+row.station.astype(str))\n",
    "    df = df.drop(columns=['cruise'])\n",
    "    df = df.groupby(['station', 'date', 'depth']).mean().reset_index()\n",
    "\n",
    "    df['p'] = gsw.p_from_z(-df.depth, df.lat)\n",
    "    df['SA'] = gsw.SA_from_SP(df.sal, df.p, df.lon, df.lat)\n",
    "    df['CT'] = gsw.CT_from_pt(df.SA, df.temp)\n",
    "    df['sigth'] = gsw.sigma0(df.SA, df.CT)\n",
    "\n",
    "    df = df.assign(database='codispoti')\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [include!!] Arrigo 2017, SUBICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arrigoetal():\n",
    "    df = pd.read_csv('/Users/doppler/database/arrigo2017/SUBICE_high_NO3_hy1.csv', \n",
    "                     na_values=-999, skiprows=9, header=[0,1],\n",
    "                    ).replace(to_replace=-999, value=np.nan)\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "    df = df.loc[df.BTLDPTH<=10].groupby('STNNBR').mean().reset_index()\n",
    "\n",
    "    df.DATE = df.DATE.apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "\n",
    "    renamedict = dict(DATE='date',LATITUDE='latitude', LONGITUDE='longitude', NITRAT='nitrate', DEPTH='depth', STNNBR='station')\n",
    "\n",
    "    df = df.rename(columns=renamedict)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [exclude?] Arcticnet etc (Pierre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define load_ routine"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = (pd.read_excel('/Users/doppler/database/arcticnet-nutrients-pierre/NUT-ArcticNet_NOW_CATS_Coupel.xlsx')\n",
    "      .drop(columns=['Bot.Depth', 'Silicate', 'Phosphate', 'Fluo'])\n",
    "     )\n",
    "df = (df\n",
    "      .rename(columns=dict(\n",
    "    Station='station', Depth='depth', Longitude='lon', Latitude='lat', \n",
    "    Nitrate='nitrate', Temperature='temp', Salinity='sal'))\n",
    "      .dropna(subset=['nitrate'])\n",
    "     )\n",
    "\n",
    "df.Day = df.Day.fillna(15)\n",
    "df['date'] = pd.to_datetime(df.Year*10000+df.Month*100+df.Day, format='%Y%m%d')\n",
    "\n",
    "df = df.assign(station=lambda row: row.station.astype(str))\n",
    "\n",
    "df = df.groupby(['station', 'date', 'depth']).mean().reset_index()\n",
    "\n",
    "df['p'] = gsw.p_from_z(-df.depth, df.lat)\n",
    "df['SA'] = gsw.SA_from_SP(df.sal, df.p, df.lon, df.lat)\n",
    "df['CT'] = gsw.CT_from_pt(df.SA, df.temp)\n",
    "df['sigth'] = gsw.sigma0(df.SA, df.CT)\n",
    "\n",
    "df = df.drop(columns=['Year', 'Month', 'Day'])\n",
    "df = df.assign(database='arcticnet')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and postprocess data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all nutrient databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = [\n",
    "    load_codispotietal(),\n",
    "    load_arrigoetal(),\n",
    "]\n",
    "\n",
    "df = pd.concat(dfl, sort=False).reset_index()\n",
    "df.to_pickle('../data/no3-compilation/tmp_compiled.pandas')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i, j in itertools.permutations(range(3),2):\n",
    "    print(i, j, np.setdiff1d(dfl[i].columns, dfl[j].columns))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## station-wise depth interpolation [takes time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/no3-compilation/tmp_compiled.pandas')\n",
    "\n",
    "def groupwise_interp(df):\n",
    "    if df.depth.min()>20:\n",
    "        return None\n",
    "    else:\n",
    "        bins = np.arange(-1,301.1,2)\n",
    "        labels = bins[:-1]+np.diff(bins)/2\n",
    "        df.depth = pd.to_numeric(pd.cut(df.depth, bins=bins, labels=labels))\n",
    "        df = (df\n",
    "              .dropna(subset=['depth'])\n",
    "              .groupby('depth').mean()\n",
    "              .reindex(index=labels)\n",
    "             )\n",
    "        return df.interpolate(method='linear', limit_area='inside').fillna(method='bfill').drop(\n",
    "            columns=['station', 'date', 'depth'], errors='ignore')\n",
    "    \n",
    "df = (df.groupby(['database', 'station', 'date'])\n",
    "      .apply(groupwise_interp)\n",
    "      .reset_index()\n",
    "     )\n",
    "\n",
    "df.to_pickle('../data/no3-compilation/tmp_compiled-interpolated.pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive per-profile quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/no3-compilation/tmp_compiled-interpolated.pandas')\n",
    "df = df.loc[df.database.isin(['codispoti', 'arcticnet'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ml (sigth, depth, delta_sigth_crit=0.1):\n",
    "    \"\"\"\n",
    "    Find mixed layer with sfc. density+0.1 kg/m3 criterion.\n",
    "    \"\"\"\n",
    "    index = np.where(sigth>np.nanmean(sigth.iloc[:5]) + delta_sigth_crit)[0]\n",
    "    if len(index)>0:\n",
    "        return depth.iloc[index[0]].astype(float)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nitracline(no3, depth):\n",
    "    no3sfc = no3.loc[depth<=10].mean()\n",
    "    index = np.where(no3>no3sfc+1.)[0]\n",
    "    if len(index)>0:\n",
    "        return depth.iloc[index[0]].astype(float)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df=df.merge(pd.DataFrame(\n",
    "            df.groupby(df.station).apply(lambda g: find_ml(g.sigth, g.depth)),\n",
    "            columns=['mld']),on='station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nitracline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two relevant nitraclines:\n",
    "1. `nc0`: Biological, when NO3 jumps over say 1uM\n",
    "1. `nc`: Physical, when NO3 jumps over say sfc.NO3+1uM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nitracline0(no3, depth, no3crit=1.0):\n",
    "    index = np.where(no3>=no3crit)[0]\n",
    "    if len(index)>0:\n",
    "        return depth.iloc[index[0]].astype(float)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def find_nitracline(no3, depth):\n",
    "    no3sfc = no3.loc[depth<=10].mean()\n",
    "    index = np.where(no3>no3sfc+1.)[0]\n",
    "    if len(index)>0:\n",
    "        return depth.iloc[index[0]].astype(float)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "gb = df.groupby(df.station)\n",
    "df = df.merge(pd.DataFrame(\n",
    "    dict(nc0=gb.apply(lambda g: find_nitracline0(g.nitrate, g.depth, 1)),\n",
    "         nc   =gb.apply(lambda g: find_nitracline(g.nitrate, g.depth))\n",
    "        )),on='station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [unused] Nitrate vs density gradient strength"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def gradient_strength(no3, sigma, depth, from_depth, layer_thickness=20):\n",
    "    \"\"\"\n",
    "    Calculate gradient ratio between no3 and sigma \n",
    "    over depth range [from_depth, from_depth+layer_thickness].\n",
    "    \"\"\"\n",
    "    nc_depth_range = (from_depth<=depth) & (depth<=from_depth+layer_thickness)\n",
    "    if len(no3)==0 or not np.any(nc_depth_range):\n",
    "        return np.nan\n",
    "    else:\n",
    "        def slope(x, y, x_range):\n",
    "            return np.polyfit(x.loc[x_range], y.loc[x_range].sort_values(), 1)[0]\n",
    "\n",
    "        no3_grad = slope(depth, no3, nc_depth_range)\n",
    "        sigma_grad = slope(depth, sigma, nc_depth_range)\n",
    "\n",
    "        return no3_grad/sigma_grad\n",
    "\n",
    "gb = df.groupby(df.station)\n",
    "df = df.merge(pd.DataFrame(\n",
    "    dict(#no3sig_nc0_1=gb.apply(lambda g: gradient_strength(g.nitrate, g.sigth, g.depth, g.nc0_1, 20)),\n",
    "         #no3sig_nc0_2=gb.apply(lambda g: gradient_strength(g.nitrate, g.sigth, g.depth, g.nc0_2, 20)),\n",
    "         no3sig_nc=gb.apply(lambda g: gradient_strength(g.nitrate, g.sigth, g.depth, g.nc, 30)),\n",
    "         no3sig_mld=gb.apply(lambda g: gradient_strength(g.nitrate, g.sigth, g.depth, g.mld, 30))\n",
    "        )),\n",
    "    on='station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take one of the measures discussed earlier... (0-100m, deltasight, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigth0 = df.loc[df.depth.between(35, 45)].groupby('station').mean().sigth\n",
    "sigth_deep = df.loc[df.depth.between(95, 105)].groupby('station').mean().sigth\n",
    "\n",
    "df = df.merge(pd.DataFrame(\n",
    "    dict(delta_sigth=(sigth_deep-sigth0).values, station=sigth0.index.values)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buoyancy_freq(sigma, depth, from_depth, layer_thickness=70):\n",
    "    \"\"\"\n",
    "    Calculate buoyancy frequency\n",
    "    over depth range [from_depth, from_depth+layer_thickness].\n",
    "    \"\"\"\n",
    "    nc_depth_range = (from_depth<=depth) & (depth<=from_depth+layer_thickness)\n",
    "    if not np.any(nc_depth_range):\n",
    "        return np.nan\n",
    "    else:\n",
    "        def slope(x, y, x_range):\n",
    "            return np.polyfit(x.loc[x_range], y.loc[x_range].sort_values(), 1)[0]\n",
    "\n",
    "        return 9.81/(1e3+np.mean(sigma)) * slope(depth, sigma, nc_depth_range)\n",
    "\n",
    "gb = df.groupby(df.station)\n",
    "df = df.merge(pd.DataFrame(\n",
    "    dict(\n",
    "        N2_30_100=gb.apply(lambda g: buoyancy_freq(g.sigth, g.depth, g.mld, 30))\n",
    "    )),\n",
    "    on='station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save database"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_pickle('../data/no3-compilation/tmp_compiled-interpolated-derived-per-station.pandas')\n",
    "\n",
    "df.groupby('station').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df.copy()\n",
    "df = (df\n",
    "       .loc[df.depth==df.depth.min()]\n",
    "#        [['date', 'lon', 'lat', 'station', 'nitrate', 'nc', 'nc0_1', 'nc0_2', 'mld', 'SA', 'CT', \n",
    "#          'sigth', 'no3sig']]#, 'no3sig_nc0_1', 'no3sig_nc0_2']]\n",
    "       .rename(columns=dict(nitrate='ntr0'))\n",
    "      )\n",
    "\n",
    "dfp = dfp.merge(df[['station', 'ntr0']], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('../data/no3-compilation/tmp_compiled-interpolated-derived-per-station.pandas')\n",
    "dfp.to_pickle('../data/no3-compilation/tmp_compiled-interpolated-derived.pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off Peralta-Ferriz & Woodgate and Codispoti et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [\n",
    "    ('Chukchi Sea', smoothen(box(-180, 68, -155, 76))), \n",
    "    ('Southern Beaufort', smoothen(box(-155, 68, -115, 72))), \n",
    "    ('Canada Basin', smoothen(box(-155, 72, -130, 84))), \n",
    "    ('Makarov Basin', smoothen(box(-180, 83.5, -50, 90)).union(smoothen(box(140, 78, 180, 90)))), \n",
    "    ('Eurasian Basin', smoothen(box(-30, 82, 140, 90).union(box(110, 78, 140, 82)))), \n",
    "    ('Barents Sea', smoothen(box(15, 75, 60, 80).union(box(15, 70.5, 55, 75)))),\n",
    "    ('Baffin Bay', smoothen(box(-65, 66, -45, 78))),\n",
    "    ('Canadian Archipelago', smoothen(box(-110, 66, -65, 80))), #.union(box(-100, 78, -50, 82))))\n",
    "    ('Fram Strait (East)', smoothen(box(-5, 75, 15, 81)))\n",
    "]\n",
    "\n",
    "names, geo = zip(*d)\n",
    "\n",
    "regions = gpd.GeoDataFrame(dict(reg_name=list(names)), geometry=list(geo))\n",
    "regions.crs = from_epsg(4326)\n",
    "\n",
    "# regions = regions.to_crs(from_epsg(3413))\n",
    "\n",
    "regions['reg_idx'] = range(len(d))\n",
    "\n",
    "regions.to_file('../data/regions/arctic-regions.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = gpd.read_file('../data/regions/arctic-regions.shp')\n",
    "options_bk = [\n",
    "    opts.Overlay(legend_position='left'),\n",
    "    opts.Polygons(projection=ccrs.NorthPolarStereo(), cmap='Category10', tools=['hover'], \n",
    "                  show_legend=True, \n",
    "                  width=500, aspect='equal',\n",
    "                  line_color=None, alpha=.7,\n",
    "                 ),\n",
    "    opts.NdOverlay(show_legend=True),\n",
    "]\n",
    "add_backend_to_opts(options_bk, 'bokeh')\n",
    "options_mpl = translate_options(options_bk, bokeh2mpl, override={'Polygons': {'edgecolor': 'none'}})\n",
    "\n",
    "poly = gv.Polygons(regions, kdims=['Longitude', 'Latitude'], vdims=['reg_name', 'reg_idx'])\n",
    "l = poly *gf.land * gf.coastline * graticules * isobath2000\n",
    "l = l.opts(*options_bk)\n",
    "\n",
    "fname = '../nb_fig/FIGURE_NO3-COMP_regions.png'\n",
    "hv.save(l.opts(toolbar=None, clone=True), fname, fmt='png')\n",
    "hv.save(l.opts(toolbar=None), fname, fmt='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add ancillary vars and regionalize dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/no3-compilation/tmp_compiled-interpolated-derived-per-station.pandas')\n",
    "dfp = pd.read_pickle('../data/no3-compilation/tmp_compiled-interpolated-derived.pandas')\n",
    "\n",
    "def season(timestamp):\n",
    "    mm = timestamp.month\n",
    "    if mm<=4:\n",
    "        return 'winter'\n",
    "    elif mm>=7 and mm<=9:\n",
    "        return 'summer'\n",
    "    \n",
    "def add_info(df):\n",
    "    df = (df\n",
    "          .assign(doy=df.date.dt.dayofyear)\n",
    "          .assign(month=df.date.dt.month)\n",
    "          .assign(deltanc=df.nc-df.mld)\n",
    "          .assign(deltanc0=df.nc0-df.mld)\n",
    "          .assign(year=df.date.dt.year)\n",
    "     )\n",
    "    df['season'] = df.date.apply(season)\n",
    "    return df\n",
    "\n",
    "df = add_info(df)\n",
    "\n",
    "gdf = df_to_gdf(df)\n",
    "gdf = (gpd.sjoin(gdf, regions, op='within', how='left')\n",
    "        .reset_index()\n",
    "        .drop(columns=['index_right', 'index_left', 'index'], errors='ignore')\n",
    "       )\n",
    "df = pd.DataFrame(gdf).drop(columns=['geometry'])\n",
    "\n",
    "dfp = dfp.merge(df, how='outer')\n",
    "dfp = add_info(dfp)\n",
    "\n",
    "df.to_csv('../data/no3-compilation/database-per-stn.csv', index=False)\n",
    "dfp.to_csv('../data/no3-compilation/database.csv', index=False)\n",
    "df.to_feather('../data/no3-compilation/database-per-stn.feather')\n",
    "dfp.to_feather('../data/no3-compilation/database.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
